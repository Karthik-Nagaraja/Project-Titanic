{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000         NaN    0.000000   \n",
       "50%     446.000000    0.000000    3.000000         NaN    0.000000   \n",
       "75%     668.500000    1.000000    3.000000         NaN    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "# We can use the pandas library in Python to read in the CSV file\n",
    "# This creates a pandas dataframe and assigns it to the titanic variable\n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "\n",
    "# Print the first five rows of the dataframe\n",
    "print(titanic.head(5))\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Up Missing Data\n",
    "\n",
    "When you used .describe() on the titanic dataframe on the last screen, you might have noticed that the Age column has a count of 714, while all of the other columns have a count of 891. This indicates that there are missing values in the Age column -- the count only includes non-missing (null, NA, or not a number) values.\n",
    "\n",
    "This means that the data isn't perfectly clean, so we're going to have to clean it ourselves. We don't want to have to remove the rows with missing values, because more data helps us train a better algorithm. We also don't want to ditch the entire column, because age is probably fairly important to our analysis.\n",
    "\n",
    "There are many ways to clean up missing data. One of the easiest is to fill in all of the missing values with the median of all the values in the column.\n",
    "\n",
    "We can select a single column by indexing the dataframe like a dictionary. This gives us a pandas series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Age\"]=titanic[\"Age\"].fillna(titanic[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the Sex Column to Numeric\n",
    "\n",
    "map_sex = {\"Sex\":{\"male\":0,\"female\":1}}\n",
    "titanic=titanic.replace(map_sex)\n",
    "titanic[\"Sex\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    0  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin  Embarked  \n",
       "0         A/5 21171   7.2500   NaN         0  \n",
       "1          PC 17599  71.2833   C85         1  \n",
       "2  STON/O2. 3101282   7.9250   NaN         0  \n",
       "3            113803  53.1000  C123         0  \n",
       "4            373450   8.0500   NaN         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "map_embarked = {\"Embarked\":{\"S\":0,\"C\":1,\"Q\":2}}\n",
    "titanic=titanic.replace(map_embarked)\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inear regression can be a very powerful algorithm, but it does have a few downsides:\n",
    "\n",
    "    If a column and an outcome aren't related linearly, it won't work well. For example, if older women don't survive as often except for women over 80, linear regression won't pick this up.\n",
    "    It can't give you survival probabilities -- only absolute values indicating whether or not someone will survive.\n",
    "\n",
    "We'll talk about how to address both of these issues later on. For now, we'll learn how to calculate linear regression coefficients automatically, as well as how to use multiple columns to predict an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"titanic_test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the data set\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Running Random forest classifier\n",
    " With random forests, we build hundreds of trees with slightly randomized input data, and slightly randomized split points. Each tree in a random forest gets a random subset of the overall training data. The algorithm performs each split point in each tree on a random subset of the potential columns to split on. By averaging the predictions of all of the trees, we get a stronger overall prediction and minimize overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785634118967\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# Compute the accuracy score for all of the cross validation folds; this is much simpler than what we did before\n",
    "kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81593714927\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross-validation folds; this is much simpler than what we did before\n",
    "kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Passengers' Titles With a Regular Expression\n",
    "\n",
    "We can extract the passengers' titles from their names. The titles take the form of Master., Mr., Mrs., etc. There are a few very common titles, and a \"long tail\" of titles that only one or two passengers have.\n",
    "\n",
    "First we'll extract the titles with a regular expression, and then map each unique title to an integer value.\n",
    "\n",
    "Then we'll have a numeric column that corresponds to the appropriate Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title  \n",
    "    # Titles always consist of capital and lowercase letters, and end with a period\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all of the titles, and print how often each one occurs\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer  \n",
    "# Some titles are very rare, so they're compressed into the same codes as other titles\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a Feature for Family Groups\n",
    "\n",
    "We can also generate a feature that indicates which family passengers belong to. Because survival was probably very dependent on your family and the people around you, this has a good chance of being a helpful feature.\n",
    "\n",
    "To create this feature, we'll concatenate each passenger's last name with FamilySize to get a unique family ID. Then we'll be able to assign a code to each person based on their family ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to ID\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the ID for a particular row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family ID\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the ID in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum ID from the mapping, and add 1 to it if we don't have an ID\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "# Get the family IDs with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family IDs, so we'll compress all of the families with less than three members into one code\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique ID\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the Best Features to Use\n",
    "\n",
    "Feature engineering is the most important part of any machine learning task, and there are a lot more features we could calculate. However, we also need a way to figure out which features are the best.\n",
    "\n",
    "One way to accomplish this is to use univariate feature selection. This approach essentially involves reviewing a data set column by column to identify the ones that correlate most closely with what we're trying to predict (Survived).\n",
    "\n",
    "As usual, sklearn has a function that will help us with feature selection. The SelectKBest function selects the best features from the data. We can specify how many features we want this function to select.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGvCAYAAAC+SGdKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktQAJnxJkhrg/we8fsoWlvXOFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x86e0a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform them from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores  \n",
    "# Do you see how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best features?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting\n",
    "\n",
    "Another technique that builds on decision trees is gradient boosting. Boosting involves training decision trees one after another, and feeding the errors from one tree into the next tree. This method allows each tree to build on all the ones that came before it. Using this method can lead to overfitting if we build too many trees, though. As we get above 100 trees, it's very easy to overfit and train to quirks in the data set. Because our data set is extremely small, we'll limit the tree count to 25.\n",
    "\n",
    "Another way to reduce overfitting is to limit the depth to which we can build each tree in the gradient boosting process. We'll limit the tree depth to 3 here to avoid overfitting.\n",
    "\n",
    "Let's try boosting instead of our random forest approach to see if we can improve our accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions With Multiple Classifiers\n",
    "\n",
    "One thing we can do to improve the accuracy of our predictions is ensemble different classifiers. Ensembling means generating predictions based on information from a set of classifiers, instead of just one. In practice, this means that we average their predictions.\n",
    "\n",
    "Generally speaking, the more diverse the models we ensemble, the higher our accuracy will be. Diversity means that the models generate their results from different columns, or use very different methods to generate predictions. Ensembling a random forest classifier with a decision tree probably won't work extremely well, because they're very similar. On the other hand, ensembling a linear regression with a random forest can yield very good results.\n",
    "\n",
    "One caveat with ensembling is that the classifiers we use have to be about the same in terms of accuracy. Ensembling one classifier that's much less accurate than the other will probably make the final result worse.\n",
    "\n",
    "In this case, we'll ensemble logistic regression we trained on the most linear predictors (the ones that have a linear order, as well as some correlation to Survived) with a gradient-boosted tree we trained on all of the predictors.\n",
    "\n",
    "We'll keep things simple when we ensemble. We'll average the raw probabilities (from 0 to 1) that we get from our classifiers, and then assume that anything above .5 maps to one, and anything below or equal to .5 maps to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kknagara\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:36: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross-validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold \n",
    "        # We need to use .astype(float) to convert the dataframe to all floats and avoid an sklearn error\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme&#8212;just average the predictions to get the final classification\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "        # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{\"O'Sullivan0\": 426, 'Mangan0': 620, 'Lindqvist1': 543, 'Denkoff0': 297, 'Sincock0': 813, 'Rouse0': 413, 'Berglund0': 207, 'Meo0': 142, 'Arnold-Franchi1': 49, 'Chronopoulos1': 71, 'Skoog5': 63, 'Walcroft0': 897, 'Widener2': 329, 'Pengelly0': 217, 'Goncalves0': 400, 'Andersen0': 829, 'Myhrman0': 626, 'Beane1': 456, 'Moss0': 104, 'Carlsson0': 610, 'Nicholls2': 136, 'Jussila1': 110, 'Jussila0': 483, 'Peltomaki0': 725, 'Long0': 632, 'Cassebeer0': 809, 'Portaluppi0': 858, 'Wheadon0': 33, 'Connolly0': 261, 'Hansen2': 680, 'Stephenson1': 493, 'Howard0': 862, 'Smyth0': 805, 'Davies0': 336, 'Silven2': 359, 'Vanden Steen0': 311, 'Sadowitz0': 878, 'Astor1': 571, 'Patchett0': 480, 'Denbury0': 891, 'Johanson0': 184, 'Coleridge0': 220, 'Christmann0': 87, 'Carter3': 340, 'Compton2': 665, 'Carter1': 226, 'Turkula0': 414, 'Lindeberg-Lind0': 793, 'Hassab0': 558, 'Badman0': 755, 'Aronsson0': 914, 'Saad0': 566, 'Mellors0': 208, 'Mamee0': 36, 'Dika0': 734, 'Madsen0': 119, 'Bird0': 801, 'Anderson0': 395, 'Kraeff0': 42, 'Robbins0': 468, 'Lundahl0': 522, 'Gilinski0': 490, 'Porter0': 107, 'Sdycoff0': 352, 'Green0': 204, 'Bishop1': 263, 'Sinkkonen0': 603, 'Otter0': 640, 'Dahl0': 299, 'Troutt0': 582, 'Samaan2': 48, 'Vartanian0': 877, 'Edvardsson0': 553, 'Larsson-Rondberg0': 920, 'Angheloff0': 874, 'Petroff0': 98, 'Burke0': 134, 'Cardeza1': 556, 'Hawksford0': 599, 'Somerton0': 416, 'Healy0': 248, 'Andersson0': 137, 'Fortune5': 27, 'Andersson6': 14, 'Willard0': 842, 'Johnson2': 9, 'Johnson0': 273, 'Wheeler0': 913, 'Coxon0': 91, 'Banfield0': 696, 'Wilkes1': 702, 'Rowe0': 883, 'Thomson0': 834, 'McCarthy0': 7, 'Panula5': 50, 'West3': 58, 'Kallio0': 374, 'Hoyt1': 206, 'Hoyt0': 638, 'Mannion0': 589, 'Touma2': 232, 'Futrelle1': 4, 'Jardin0': 507, 'Rosenbaum0': 827, 'Lemore0': 437, 'Davies2': 460, 'Myles0': 703, 'Reynolds0': 835, 'Robert1': 630, 'Butt0': 451, 'Wick2': 286, 'Emanuel0': 628, 'Ibrahim Shawah0': 643, 'Carbines0': 175, 'McEvoy0': 583, 'Moutal0': 75, 'Loring0': 875, 'Giles0': 896, 'Ling0': 157, 'Watson0': 552, 'Lahoud0': 443, 'Sedgwick0': 302, 'Assaf0': 711, 'Storey0': 799, 'Beesley0': 22, 'Sheerlinck0': 79, 'Hunt0': 218, 'Duquemin0': 800, 'Clifford0': 408, 'Stranden0': 602, 'Ahlin1': 40, 'Woolner0': 55, 'Caram1': 482, 'Sandstrom2': 11, 'Nakid2': 333, 'Frost0': 412, 'Moen0': 73, 'Henry0': 239, 'Persson1': 241, 'Mudd0': 671, 'Glynn0': 32, 'Bowerman1': 312, 'Douglas2': 816, 'Daniel0': 505, 'Nysten0': 132, 'Saalfeld0': 270, 'Turcin0': 173, 'Nasser1': 10, 'Waelens0': 78, 'Khalil1': 753, 'Pokrnic0': 863, 'Homer0': 502, 'Graham1': 242, 'Graham0': 699, 'Brown0': 177, 'Dorking0': 256, 'Clark1': 851, 'Bing0': 72, 'Blank0': 191, 'Tornquist0': 245, 'Olsen1': 180, 'Olsen0': 144, 'Lane0': 815, 'Davison1': 304, 'Butler0': 544, 'Calderhead0': 575, 'Cor0': 530, 'Kalvik0': 534, 'Hagland1': 386, 'Boulos2': 131, 'Boulos0': 497, 'Niklasson0': 855, 'Leitch0': 496, 'Endres0': 581, 'Fleming0': 275, 'Natsch1': 247, 'Garfirth0': 614, 'MacKay0': 854, 'Blackwell0': 300, 'Thayer2': 461, 'Holm0': 657, 'Rekic0': 105, 'Allen0': 5, 'Hendekovic0': 282, 'Svensson0': 424, 'Montvila0': 698, 'Perreault0': 441, 'Francatelli0': 278, 'Gronnestad0': 621, 'Maisner0': 399, 'Radeff0': 537, 'Daly0': 432, 'Hiltunen2': 846, 'Conlon0': 921, 'Jarvis0': 488, 'Bowen0': 515, 'Holthen0': 770, 'Risien0': 453, 'Walker0': 436, 'Klaber0': 578, 'Wright0': 466, 'Artagaveytia0': 419, 'Navratil2': 138, 'Hays2': 658, 'Seward0': 383, 'Hays0': 279, 'Appleton2': 478, 'Fry0': 654, 'Romaine0': 171, 'de Messemaeker1': 469, 'Gustafsson0': 331, 'Gustafsson2': 101, 'Lulic0': 659, 'Beckwith2': 225, 'Sutton0': 516, 'Cunningham0': 355, 'Carver0': 780, 'Drapkin0': 790, 'Hocking0': 840, 'Behr0': 700, 'Cotterill0': 911, 'Rothschild1': 434, 'Stankovic0': 257, 'Abelson1': 277, 'Shaughnessy0': 727, 'Lobb1': 230, 'Shellard0': 423, 'Knight0': 593, 'Silverthorne0': 572, \"O'Leary0\": 535, 'Saundercock0': 13, 'Baxter1': 114, 'Elias0': 624, 'Nankoff0': 598, 'Mitkoff0': 533, 'Lines1': 677, 'Burns0': 298, \"O'Driscoll0\": 47, 'Beattie0': 778, 'Cumings1': 2, 'Culumovic0': 674, 'Thomas2': 769, 'Penasco y Castellana1': 276, 'Parkes0': 251, 'Duff Gordon1': 467, 'Sloper0': 24, 'Strom2': 228, 'Ringhini0': 327, 'Barah0': 616, 'Richards2': 350, 'Crease0': 67, 'Cavendish1': 600, 'Earnshaw1': 797, 'Hilliard0': 795, 'Stengel1': 766, 'Lamb0': 752, 'Bjornstrom-Steffansson0': 371, 'Masselmani0': 20, 'Moubarek2': 65, 'Turja0': 555, 'Goldsmith2': 154, 'Fahlstrom0': 210, 'Holverson1': 35, 'Herman3': 510, \"O'Donoghue0\": 756, 'Nenkoff0': 205, 'Nysveen0': 292, 'Eklund0': 617, 'Paulner0': 487, 'Whabee0': 895, 'Nilsson0': 284, 'Dick1': 563, 'Dantcheff0': 639, 'Swift0': 682, 'Humblen0': 570, 'Matinoff0': 798, 'McCormack0': 661, 'Duane0': 253, 'Ekstrom0': 121, 'Wiseman0': 366, 'Smith0': 160, 'Krekorian0': 881, 'Nourney0': 922, 'Abrahamsson0': 850, 'Saether0': 928, 'Marvin1': 604, 'Cleaver0': 576, 'Gill0': 683, 'Cameron0': 193, 'Augustsson0': 663, 'Petranec0': 97, 'Stahelin-Maeglin0': 523, 'McDermott0': 80, 'Wittevrongel0': 873, 'Dean3': 90, 'Laleff0': 691, 'Meyer0': 649, 'Meyer1': 34, 'Chaudanson0': 733, 'Johansson0': 100, 'Cacic0': 405, 'Minkoff0': 740, 'Ryerson4': 280, 'Becker3': 167, 'Murphy0': 824, 'Tobin0': 627, 'Murphy1': 219, 'Goldschmidt0': 93, 'Kink2': 68, 'Gilbert0': 917, 'Weisz1': 125, 'Hold1': 215, 'Lingane0': 821, 'Stanley0': 420, 'Doyle0': 748, 'Minahan2': 222, 'Toufik0': 450, 'Minahan1': 354, 'Olsvigen0': 559, 'Odahl0': 307, 'Coleff0': 435, 'Case0': 750, 'Vestrom0': 15, 'Brobeck0': 782, 'Hood0': 70, 'Yousseff0': 421, 'Evans0': 776, 'Mardirosian0': 869, 'Abrahim0': 706, 'Barton0': 109, 'Icard0': 61, 'Smiljanic0': 148, 'Harmer0': 634, 'Collett0': 826, 'Ford4': 84, 'Maioni0': 428, 'Abbing0': 675, 'Ford0': 870, 'Kink-Heilmann4': 918, 'Oreskovic0': 347, 'Sundman0': 356, 'Kink-Heilmann2': 168, 'Lehmann0': 339, 'Dennis0': 288, 'del Carlo1': 316, 'Najib0': 690, 'Ohman0': 465, 'McCoy2': 272, 'Osen0': 129, 'Pernot0': 166, 'Kent0': 415, 'Turpin1': 41, 'Zabour1': 108, 'Alexander0': 650, 'Palsson4': 8, 'Hogeboom1': 618, 'Keefe0': 404, 'Eustis1': 422, 'Marechal0': 669, 'White0': 879, 'Uruchurtu0': 30, 'Riordan0': 923, 'Downton0': 485, 'Ilmakangas1': 591, 'Corn0': 147, 'Maybery0': 817, 'Parrish1': 236, 'Jerwan0': 406, 'Aks1': 678, 'Lam0': 565, 'Slayter0': 290, 'Weir0': 567, 'Frolicher2': 454, 'Danbom2': 365, 'Alhomaki0': 670, 'Brown2': 548, 'Faunthorpe1': 53, 'Wilson0': 908, 'Givard0': 195, 'Leyson0': 213, 'Potter1': 692, 'Emir0': 26, 'Slocovski0': 85, 'Celotti0': 86, 'Rood0': 169, 'Betros0': 330, 'Spinner0': 785, 'Larsson0': 211, 'Goldsmith0': 723, 'Andreasson0': 88, 'Vande Walle0': 183, 'Zakarian0': 788, 'Morley0': 396, 'Trout0': 344, 'Jefferys2': 716, 'Lundstrom0': 893, 'Thorneycroft1': 372, 'Shorney0': 92, 'Attalah0': 111, 'Ward0': 235, 'Bowenur0': 783, 'Tucker0': 738, 'Yousif0': 310, 'Wiklund1': 325, 'Thorne0': 233, 'Warren0': 861, 'Warren1': 320, 'Milling0': 398, 'Delalic0': 828, 'Rommetvedt0': 545, 'Pickard0': 370, 'Torfa0': 812, 'Hassan0': 592, 'Heininen0': 655, 'Vendel0': 844, 'Miles0': 745, 'Bazzani0': 200, 'Plotcharsky0': 335, 'Lindell1': 503, 'Caldwell2': 76, 'Meanwell0': 474, 'Bystrom0': 684, 'Harrison0': 238, 'Goldenberg1': 388, 'Webber0': 117, 'Shelley1': 693, 'Mayne0': 577, 'Honkanen0': 198, 'Karlsson0': 410, 'Ismay0': 909, 'Asplund0': 837, 'Calic0': 153, 'Eitemiller0': 538, 'Chapman0': 568, 'Asplund6': 25, 'Dulles0': 888, 'Peuchen0': 385, 'Cairns0': 244, 'Midtsjo0': 857, 'Bailey0': 611, 'Hanna0': 268, 'Garside0': 481, 'Mernagh0': 179, 'Staneff0': 74, 'Toomey0': 393, 'Howard1': 710, 'Canavan0': 425, 'Kennedy0': 781, 'Dakic0': 560, 'Bucknell0': 728, 'Foo0': 529, 'Pasic0': 666, 'Veal0': 819, 'Novel0': 57, 'Lemberopolous0': 673, 'Hocking4': 625, 'Aldworth0': 747, 'Moran1': 106, 'Abbott2': 252, 'Kvillner0': 377, 'Elias2': 309, 'Gracie0': 786, 'Doharr0': 476, 'Norman0': 472, 'Pearce0': 806, 'Braf0': 764, 'Leader0': 641, 'Smart0': 402, 'White1': 99, 'Gale1': 348, 'de Brito0': 890, 'Swane0': 773, 'Doling1': 95, 'Sap0': 720, 'Moor1': 607, 'Pedersen0': 758, 'Taussig2': 237, 'Pinsky0': 174, 'Mulvihill0': 739, 'Gallagher0': 573, 'Lockyer0': 901, 'Thomas0': 777, 'Markun0': 694, 'de Mulder0': 258, 'Kassem0': 444, 'Yrois0': 182, 'Lundin0': 802, 'Kantor1': 96, 'Sobey0': 126, 'Shutes0': 506, 'Brocklebank0': 509, 'Parker0': 866, 'Cherry0': 234, 'Dooley0': 701, 'Hedman0': 646, 'Madigan0': 181, 'Ovies y Rodriguez0': 742, 'Strilic0': 904, 'Parr0': 524, 'Campbell0': 401, 'Mullens0': 569, 'Maguire0': 889, 'Charters0': 363, 'Baimbrigge0': 822, 'Jonsson0': 477, 'Ostby1': 54, 'Brady0': 715, 'Wilhelms0': 551, 'Williams1': 145, 'Williams0': 18, 'Maenpaa0': 221, 'Stone0': 662, 'Birnbaum0': 761, 'Lennon1': 46, 'Olsson0': 254, 'Harknett0': 214, 'Horgan0': 508, 'Hart0': 353, 'Hart2': 283, 'Ilett0': 82, 'Hellstrom0': 810, 'Naughton0': 924, 'Baumann0': 156, 'Youseff0': 185, 'Reeves0': 240, 'Thomas1': 645, 'Jensen0': 527, 'Jensen1': 584, 'Cribb1': 150, 'Ross0': 486, 'Andersen-Jensen1': 176, 'Gilnagh0': 146, 'Moran0': 6, 'Coelho0': 123, 'Razi0': 679, 'Harbeck0': 910, 'Shine0': 775, 'Reuchlin0': 660, 'Kiernan1': 196, 'Partner0': 295, 'Jermyn0': 322, 'Salkjelsvik0': 103, 'Lahtinen2': 281, 'Drew2': 358, 'Hosono0': 260, 'Danoff0': 289, 'van Billiard2': 143, 'Bracken0': 203, 'Dintcheff0': 787, 'Coutts2': 305, 'Connors0': 113, 'Colbert0': 919, 'Jenkin0': 69, 'Hodges0': 586, 'Vovk0': 442, 'Omont0': 825, 'Backstrom1': 188, 'Devaney0': 44, 'Backstrom3': 83, 'Barbara1': 317, \"O'Connell0\": 520, 'Vande Velde0': 608, 'Reynaldo0': 380, 'Isham0': 163, 'Peruschitz0': 807, \"O'Brien0\": 463, \"O'Brien1\": 170, 'Giglio0': 130, 'Ponesell0': 644, 'Nicola-Yarred1': 39, 'Angle1': 439, 'Mellinger1': 246, 'Newell1': 197, 'Davidson3': 759, 'Lang0': 431, 'Davidson1': 549, 'Richard0': 127, 'Oxenham0': 868, 'Ball0': 293, 'Nasr0': 872, 'Schabert1': 779, 'Drazenoic0': 122, 'Robins1': 124, 'Richards5': 376, 'Chevre0': 726, 'Salander0': 852, 'Stanton0': 774, 'Nosworthy0': 51, 'Moore0': 116, 'Newsom2': 128, 'Sjoblom0': 635, 'Zimmerman0': 364, 'Kenyon1': 392, 'Fynney0': 21, 'Laroche3': 43, 'Renouf1': 409, 'Silvey1': 375, 'Riihivouri0': 905, 'Christy2': 484, 'Renouf3': 588, 'Pulbaum0': 730, 'Theobald0': 612, 'Mahon0': 833, 'Pekoniemi0': 112, 'Spector0': 926, 'Bateman0': 140, 'Spencer1': 31, 'Gibson1': 906, 'Allison3': 269, 'Cohen0': 186, 'Dimic0': 306, 'Crafton0': 796, 'Karnes0': 849, 'Bourke2': 172, 'Farthing0': 447, 'Hamalainen2': 224, 'Karaic0': 504, 'Hyman0': 848, 'Pettersson0': 648, 'Landergren0': 328, 'Willey0': 532, 'Braund1': 1, 'Rosenshine0': 886, 'Slemen0': 652, 'Snyder1': 709, 'Jalsevac0': 390, 'Harder1': 324, 'Oliva y Ocana0': 927, 'Elsbury0': 494, 'Assam0': 885, 'Carrau0': 81, 'Dyker1': 757, 'Sutehall0': 697, 'Everett0': 839, 'Sawyer0': 554, 'Vander Planke1': 19, 'Peters0': 557, 'Vander Planke3': 794, 'Vander Planke2': 38, 'Dahlberg0': 695, 'Sharp0': 462, 'Sirota0': 667, 'McCaffry0': 864, 'Demetri0': 751, 'Barkworth0': 521, 'Buckley0': 771, 'Mack0': 623, 'Petersen0': 784, 'Sjostedt0': 212, 'Slabenoff0': 499, 'Dodge2': 382, 'Mineff0': 266, 'Lithman0': 811, 'Hocking3': 449, 'Hickman2': 115, 'Perkin0': 194, 'Sweet0': 841, 'Stewart0': 64, 'Giles1': 681, 'Lindblom0': 250, 'Meek0': 357, 'Morrow0': 470, 'Newell2': 539, 'Beavan0': 326, 'Balkic0': 688, 'van Melkebeke0': 687, 'Foreman0': 387, 'Hampe0': 378, 'Birkeland0': 351, 'Ware0': 903, 'Mockler0': 315, 'Millet0': 391, 'Peter2': 120, 'Yasbeck1': 512, 'Osman0': 642, 'Buss0': 337, 'Sunderland0': 202, 'Fillbrook0': 892, 'Candee0': 836, 'Lefebre4': 162, 'Bryhl1': 590, 'McGovern0': 314, 'Nye0': 66, 'Davis0': 525, 'Aubart0': 323, 'Spedden2': 287, 'Harris0': 201, 'Harris1': 62, 'McGough0': 433, 'Hewlett0': 16, 'Cann0': 37, 'Makinen0': 763, 'Mallet2': 656, 'Mock1': 717, 'Kirkland0': 517, 'Ridsdale0': 446, 'Connaghton0': 605, 'Stoytcheff0': 475, 'Ashby0': 915, 'Hippach1': 294, 'Dowdell0': 77, 'Klasen2': 161, 'Assaf Khalil0': 712, 'Rothes0': 613, 'Nancarrow0': 765, \"O'Connor0\": 394, 'Ivanoff0': 597, 'Clarke1': 367, 'Scanlan0': 403, 'Daniels0': 791, 'Troupiansky0': 595, 'Johansson Palmquist0': 768, 'Geiger0': 743, 'Reed0': 227, 'Rintamaki0': 492, 'Chambers1': 587, 'Malachard0': 876, 'Haas0': 265, 'Corey0': 737, 'Greenfield1': 94, 'Brandeis0': 808, 'Jansson0': 341, 'Deacon0': 831, 'McNeill0': 838, 'Gheorgheff0': 362, 'Lindstrom0': 847, 'Mitchell0': 550, 'Dibden0': 899, 'Bonnell0': 12, 'Murdlin0': 491, 'Lester0': 651, 'Van Impe2': 361, 'Simonius-Blumer0': 531, 'Torber0': 501, 'Linehan0': 843, 'Gillespie0': 585, 'Naidenoff0': 259, 'McNamee1': 601, 'de Pelsmaeker0': 255, 'Quick2': 429, 'Bentham0': 856, 'Byles0': 139, 'Salomon0': 820, 'Sage10': 149, 'Baccos0': 845, 'Frolicher-Stehli2': 489, 'Wirz0': 704, 'Peduzzi0': 389, 'Chibnall1': 155, 'Hakkarainen1': 133, 'Watt0': 151, 'Sirayanian0': 60, 'Leonard0': 165, 'Bissette0': 243, 'Adahl0': 319, 'LeRoy0': 452, 'Tomlin0': 653, 'Smith1': 729, 'Kimball1': 513, 'Gavey0': 511, 'Mionoff0': 102, 'Farrell0': 445, 'Goodwin7': 59, 'Strom1': 187, 'Ali0': 192, 'Funk0': 313, 'Van der hoef0': 158, 'Roebling0': 686, 'Stokes0': 898, 'Simmons0': 473, 'Asim0': 318, 'Rosblom2': 231, 'Willer0': 772, 'Saade0': 865, 'Lovell0': 209, 'Phillips0': 368, 'Phillips1': 818, 'Lyntakoff0': 859, 'Leinonen0': 526, 'Jones0': 708, 'Markoff0': 676, 'Levy0': 264, 'Frauenthal2': 540, 'Frauenthal1': 296, 'Rheims0': 871, 'Finoli0': 830, 'Wenzel0': 853, 'Baclini3': 384, 'Johannesen-Bratthammer0': 381, 'Jonkoff0': 609, 'Williams-Lambert0': 308, 'Serepeca0': 672, 'Cornell2': 746, 'Longley0': 518, \"O'Dwyer0\": 28, 'Enander0': 887, 'Chisholm0': 860, 'Kelly0': 271, 'Douglas1': 457, 'Crosby2': 455, 'Niskanen0': 345, 'Collander0': 301, 'Gee0': 397, 'Tikkanen0': 334, 'McKane0': 342, 'Molson0': 418, 'Bostandyeff0': 519, 'Ayoub0': 631, 'Rogers0': 45, 'Chip0': 668, 'Cook0': 546, 'Stead0': 229, 'Nesson0': 882, 'Peacock2': 804, 'Soholt0': 580, 'Moraweck0': 285, 'Rasmussen0': 823, 'Barry0': 754, 'Bidois0': 332, 'Taylor1': 547, 'Bradley0': 430, 'Fischer0': 561, 'Barber0': 262, 'Julian0': 900, 'Albimona0': 189, 'Hale0': 164, 'Flynn0': 369, 'Sadlier0': 338, 'Keane0': 274, 'Pallas y Castello0': 907, 'Young0': 291, 'Lindahl0': 223, 'Rugg0': 56, 'Johnston3': 633, 'Guest0': 760, 'Petterson1': 379, 'Pavlovic0': 440, 'Abelseth0': 732, 'Sivic0': 471, 'Hirvonen1': 411, 'Hirvonen2': 705, 'Allum0': 664, 'Tenglin0': 762, 'Head0': 832, 'Keeping0': 744, 'Straus1': 749, 'Harrington0': 500, 'Borebank0': 803, 'Beauchamp0': 792, 'Windelov0': 417, 'Lesurer0': 596, 'Pain0': 343, 'Lievens0': 622, 'Fox0': 303, 'Lewy0': 267, 'McMahon0': 118, 'Botsford0': 894, 'Greenberg0': 579, 'Heikkinen0': 3, 'Henriksson0': 925, 'Mangiavacchi0': 731, 'Louch1': 373, 'Chapman1': 495, 'Schmidt0': 789, 'Berriman0': 594, 'Franklin0': 722, 'Bjorklund0': 736, 'Moussa0': 321, 'Payne0': 916, 'Ware1': 867, 'Hee0': 721, 'Ryan0': 438, \"O'Keefe0\": 902, 'Roth0': 719, 'Madill1': 562, 'Duran y More1': 685, 'Foley0': 767, 'Bengtsson0': 152, 'Harper1': 52, 'Kilgannon0': 629, 'Badt0': 541, 'Salonen0': 448, 'Guggenheim0': 636, 'Widegren0': 349, 'Brewe0': 619, 'Sivola0': 159, 'Nirva0': 615, 'Rice5': 17, 'Andrews0': 647, 'Andrews1': 249, 'Collyer2': 216, 'Strandberg0': 407, 'Colley0': 542, 'Nicholson0': 458, 'Nieminen0': 741, 'Chaffee1': 89, 'Gaskell0': 637, 'McCrie0': 814, 'Wells2': 606, 'Leeni0': 464, 'Todoroff0': 29, 'Flegenheim0': 713, 'McCrae0': 735, 'Adams0': 346, 'Corbett0': 724, 'Hipkins0': 912, 'Pears1': 141, 'Sagesser0': 528, 'Carr0': 190, 'Ilieff0': 707, 'McGowan0': 23, 'Hansen0': 514, 'Hansen1': 574, 'Karun1': 564, 'Rush0': 479, 'Katavelas0': 718, 'Matthews0': 360, 'Laitinen0': 427, 'Kreuchen0': 884, 'Hagardon0': 880, 'Daher0': 714, 'Padro y Manent0': 459, 'Andrew0': 135, 'Vander Cruyssen0': 689, 'Jacobsohn1': 199, 'Lurette0': 178, 'Jacobsohn3': 498, 'Hegarty0': 536}\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now we add the family size column\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family IDs\n",
    "# We'll use the same IDs we used earlier\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12: Predicting on the Test Set\n",
    "\n",
    "We have some better predictions now, so let's create another submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Thoughts\n",
    "\n",
    "Now we have a submission! It should earn you a score of .799 on the leaderboard. You can generate a submission file with submission.to_csv(\"kaggle.csv\", index=False).\n",
    "\n",
    "There's still more work you can do in feature engineering:\n",
    "\n",
    "    Try using features related to the cabins.\n",
    "    See if any family size features might help. Do the number of women in a family make the entire family more likely to survive?\n",
    "    Does the national origin of the passenger's name have anything to do with survival?\n",
    "\n",
    "There's also a lot more we can do on the algorithm side:\n",
    "\n",
    "    Try the random forest classifier in the ensemble.\n",
    "    A support vector machine might work well with this data.\n",
    "    We could try neural networks.\n",
    "    Boosting with a different base classifier might work better.\n",
    "\n",
    "And with ensembling methods:\n",
    "\n",
    "    Could majority voting be a better ensembling method than averaging probabilities?\n",
    "\n",
    "This data set is very easy to overfit on; there isn't a lot of data, so you'll be grinding for small accuracy gains. You could also try a different Kaggle competition that has more data and richer features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
